package services

import (
	"context"
	"fmt"
	"sort"
	"time"

	"github.com/codelieche/cronjob/apiserver/pkg/core"
	"github.com/codelieche/cronjob/apiserver/pkg/store"
	"github.com/codelieche/cronjob/apiserver/pkg/utils/logger"
	"github.com/google/uuid"
	"go.uber.org/zap"
	"gorm.io/gorm"
)

// WorkflowStatsService Workflowç»Ÿè®¡æœåŠ¡
//
// å®ç° core.WorkflowStatsService æ¥å£
type WorkflowStatsService struct {
	db            *gorm.DB
	statsStore    *store.WorkflowStatsStore
	executeStore  core.WorkflowExecuteStore
	workflowStore core.WorkflowStore
}

// NewWorkflowStatsService åˆ›å»ºServiceå®ä¾‹
func NewWorkflowStatsService(
	db *gorm.DB,
	statsStore *store.WorkflowStatsStore,
	executeStore core.WorkflowExecuteStore,
	workflowStore core.WorkflowStore,
) *WorkflowStatsService {
	return &WorkflowStatsService{
		db:            db,
		statsStore:    statsStore,
		executeStore:  executeStore,
		workflowStore: workflowStore,
	}
}

// DailyAgg æ¯æ—¥èšåˆæ•°æ®ï¼ˆè¾…åŠ©ç»“æ„ï¼‰
type DailyAgg struct {
	Date     string
	Total    int
	Success  int
	Failed   int
	Canceled int
}

// AggregateDailyStats èšåˆæŒ‡å®šæ—¥æœŸçš„ç»Ÿè®¡æ•°æ®
// ä» workflow_executes è¡¨èšåˆåˆ° workflow_stats_daily è¡¨
func (s *WorkflowStatsService) AggregateDailyStats(ctx context.Context, date time.Time) error {
	startTime := time.Date(date.Year(), date.Month(), date.Day(), 0, 0, 0, 0, time.Local)
	endTime := startTime.Add(24 * time.Hour)

	logger.Info("å¼€å§‹èšåˆWorkflowç»Ÿè®¡æ•°æ®",
		zap.String("date", date.Format("2006-01-02")),
		zap.Time("start_time", startTime),
		zap.Time("end_time", endTime))

	// ğŸ”¥ ä½¿ç”¨åŸç”ŸSQLèšåˆæŸ¥è¯¢ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
	// æŒ‰ workflow_id, team_id åˆ†ç»„èšåˆ
	query := `
		SELECT 
			workflow_id,
			team_id,
			COUNT(*) as total_executes,
			SUM(CASE WHEN status = 'success' THEN 1 ELSE 0 END) as success_executes,
			SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END) as failed_executes,
			SUM(CASE WHEN status = 'canceled' THEN 1 ELSE 0 END) as canceled_executes,
			AVG(CASE 
				WHEN time_start IS NOT NULL AND time_end IS NOT NULL 
				THEN TIMESTAMPDIFF(SECOND, time_start, time_end) 
				ELSE NULL 
			END) as avg_duration,
			MIN(CASE 
				WHEN time_start IS NOT NULL AND time_end IS NOT NULL 
				THEN TIMESTAMPDIFF(SECOND, time_start, time_end) 
				ELSE NULL 
			END) as min_duration,
			MAX(CASE 
				WHEN time_start IS NOT NULL AND time_end IS NOT NULL 
				THEN TIMESTAMPDIFF(SECOND, time_start, time_end) 
				ELSE NULL 
			END) as max_duration,
			AVG(total_steps) as avg_total_steps,
			AVG(success_steps) as avg_success_steps,
			AVG(failed_steps) as avg_failed_steps,
			SUM(CASE WHEN trigger_type = 'manual' THEN 1 ELSE 0 END) as manual_triggers,
			SUM(CASE WHEN trigger_type = 'api' THEN 1 ELSE 0 END) as api_triggers,
			SUM(CASE WHEN trigger_type = 'webhook' THEN 1 ELSE 0 END) as webhook_triggers,
			SUM(CASE WHEN trigger_type = 'schedule' THEN 1 ELSE 0 END) as schedule_triggers
		FROM workflow_executes
		WHERE created_at >= ? AND created_at < ?
			AND deleted = 0
			AND status IN ('success', 'failed', 'canceled')
		GROUP BY workflow_id, team_id
	`

	type AggResult struct {
		WorkflowID       string   `gorm:"column:workflow_id"`
		TeamID           *string  `gorm:"column:team_id"`
		TotalExecutes    int      `gorm:"column:total_executes"`
		SuccessExecutes  int      `gorm:"column:success_executes"`
		FailedExecutes   int      `gorm:"column:failed_executes"`
		CanceledExecutes int      `gorm:"column:canceled_executes"`
		AvgDuration      *float64 `gorm:"column:avg_duration"`
		MinDuration      *float64 `gorm:"column:min_duration"`
		MaxDuration      *float64 `gorm:"column:max_duration"`
		AvgTotalSteps    *float64 `gorm:"column:avg_total_steps"`
		AvgSuccessSteps  *float64 `gorm:"column:avg_success_steps"`
		AvgFailedSteps   *float64 `gorm:"column:avg_failed_steps"`
		ManualTriggers   int      `gorm:"column:manual_triggers"`
		ApiTriggers      int      `gorm:"column:api_triggers"`
		WebhookTriggers  int      `gorm:"column:webhook_triggers"`
		ScheduleTriggers int      `gorm:"column:schedule_triggers"`
	}

	var results []AggResult
	if err := s.db.WithContext(ctx).Raw(query, startTime, endTime).Scan(&results).Error; err != nil {
		logger.Error("èšåˆæŸ¥è¯¢å¤±è´¥", zap.Error(err))
		return err
	}

	logger.Info("èšåˆæŸ¥è¯¢å®Œæˆ", zap.Int("workflow_count", len(results)))

	// éå†èšåˆç»“æœï¼Œå†™å…¥ç»Ÿè®¡è¡¨
	for _, result := range results {
		workflowID, err := uuid.Parse(result.WorkflowID)
		if err != nil {
			logger.Error("è§£æworkflow_idå¤±è´¥", zap.String("workflow_id", result.WorkflowID), zap.Error(err))
			continue
		}

		var teamID *uuid.UUID
		if result.TeamID != nil && *result.TeamID != "" {
			tid, err := uuid.Parse(*result.TeamID)
			if err == nil {
				teamID = &tid
			}
		}

		// æŸ¥è¯¢Workflowåç§°
		workflow, err := s.workflowStore.FindByID(ctx, workflowID)
		workflowName := ""
		if err == nil {
			workflowName = workflow.Name
		}

		// æŸ¥è¯¢æ˜¯å¦å·²å­˜åœ¨ç»Ÿè®¡è®°å½•
		existingStats, err := s.statsStore.FindByWorkflowAndDate(ctx, workflowID, teamID, date)

		stats := &core.WorkflowStatsDaily{
			WorkflowID:       workflowID,
			WorkflowName:     workflowName,
			TeamID:           teamID,
			StatDate:         startTime,
			TotalExecutes:    result.TotalExecutes,
			SuccessExecutes:  result.SuccessExecutes,
			FailedExecutes:   result.FailedExecutes,
			CanceledExecutes: result.CanceledExecutes,
			ManualTriggers:   result.ManualTriggers,
			ApiTriggers:      result.ApiTriggers,
			WebhookTriggers:  result.WebhookTriggers,
			ScheduleTriggers: result.ScheduleTriggers,
		}

		// å¤„ç†å¯èƒ½ä¸ºç©ºçš„å¹³å‡å€¼
		if result.AvgDuration != nil {
			stats.AvgDuration = *result.AvgDuration
		}
		if result.MinDuration != nil {
			stats.MinDuration = *result.MinDuration
		}
		if result.MaxDuration != nil {
			stats.MaxDuration = *result.MaxDuration
		}
		if result.AvgTotalSteps != nil {
			stats.AvgTotalSteps = *result.AvgTotalSteps
		}
		if result.AvgSuccessSteps != nil {
			stats.AvgSuccessSteps = *result.AvgSuccessSteps
		}
		if result.AvgFailedSteps != nil {
			stats.AvgFailedSteps = *result.AvgFailedSteps
		}

		// å¦‚æœå·²å­˜åœ¨ï¼Œæ›´æ–°ï¼›å¦åˆ™åˆ›å»º
		if existingStats != nil {
			stats.ID = existingStats.ID
			if err := s.statsStore.Update(ctx, stats); err != nil {
				logger.Error("æ›´æ–°ç»Ÿè®¡è®°å½•å¤±è´¥",
					zap.String("workflow_id", workflowID.String()),
					zap.Error(err))
			}
		} else {
			if err := s.statsStore.Create(ctx, stats); err != nil {
				logger.Error("åˆ›å»ºç»Ÿè®¡è®°å½•å¤±è´¥",
					zap.String("workflow_id", workflowID.String()),
					zap.Error(err))
			}
		}
	}

	logger.Info("Workflowç»Ÿè®¡æ•°æ®èšåˆå®Œæˆ",
		zap.String("date", date.Format("2006-01-02")),
		zap.Int("processed_workflows", len(results)))

	return nil
}

// AggregateHistoricalStats èšåˆå†å²ç»Ÿè®¡æ•°æ®ï¼ˆæ‰¹é‡ï¼‰
// ç”¨äºåˆæ¬¡éƒ¨ç½²æˆ–è¡¥å……å†å²æ•°æ®
func (s *WorkflowStatsService) AggregateHistoricalStats(ctx context.Context, startDate, endDate time.Time) error {
	logger.Info("å¼€å§‹æ‰¹é‡èšåˆå†å²ç»Ÿè®¡æ•°æ®",
		zap.String("start_date", startDate.Format("2006-01-02")),
		zap.String("end_date", endDate.Format("2006-01-02")))

	// é€æ—¥èšåˆ
	currentDate := startDate
	successCount := 0
	failedCount := 0

	for currentDate.Before(endDate) || currentDate.Equal(endDate) {
		if err := s.AggregateDailyStats(ctx, currentDate); err != nil {
			logger.Error("èšåˆå•æ—¥æ•°æ®å¤±è´¥",
				zap.String("date", currentDate.Format("2006-01-02")),
				zap.Error(err))
			failedCount++
		} else {
			successCount++
		}
		currentDate = currentDate.AddDate(0, 0, 1)
	}

	logger.Info("æ‰¹é‡èšåˆå®Œæˆ",
		zap.Int("success_count", successCount),
		zap.Int("failed_count", failedCount))

	return nil
}

// GetSuccessRateTrend è·å–æˆåŠŸç‡è¶‹åŠ¿
// è¿”å›æœ€è¿‘Nå¤©æ¯å¤©çš„æ‰§è¡Œç»Ÿè®¡ï¼ˆtotal, success, failed, success_rateï¼‰
func (s *WorkflowStatsService) GetSuccessRateTrend(
	ctx context.Context,
	teamID *uuid.UUID,
	days int,
) (map[string]interface{}, error) {
	stats, err := s.statsStore.GetDailyStats(ctx, teamID, days)
	if err != nil {
		return nil, err
	}

	// èšåˆæ¯æ—¥æ•°æ®
	dailyMap := make(map[string]*DailyAgg)
	for _, stat := range stats {
		dateKey := stat.StatDate.Format("2006-01-02")
		if agg, exists := dailyMap[dateKey]; exists {
			agg.Total += stat.TotalExecutes
			agg.Success += stat.SuccessExecutes
			agg.Failed += stat.FailedExecutes
			agg.Canceled += stat.CanceledExecutes
		} else {
			dailyMap[dateKey] = &DailyAgg{
				Date:     dateKey,
				Total:    stat.TotalExecutes,
				Success:  stat.SuccessExecutes,
				Failed:   stat.FailedExecutes,
				Canceled: stat.CanceledExecutes,
			}
		}
	}

	// è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åº
	var trendData []map[string]interface{}
	dates := make([]string, 0, len(dailyMap))
	for date := range dailyMap {
		dates = append(dates, date)
	}
	sort.Strings(dates)

	for _, date := range dates {
		agg := dailyMap[date]
		successRate := 0.0
		if agg.Total > 0 {
			successRate = float64(agg.Success) * 100.0 / float64(agg.Total)
		}

		trendData = append(trendData, map[string]interface{}{
			"date":         agg.Date,
			"total":        agg.Total,
			"success":      agg.Success,
			"failed":       agg.Failed,
			"canceled":     agg.Canceled,
			"success_rate": fmt.Sprintf("%.1f", successRate),
		})
	}

	return map[string]interface{}{
		"data": trendData,
	}, nil
}

// GetExecutionEfficiency è·å–æ‰§è¡Œæ•ˆç‡ç»Ÿè®¡
func (s *WorkflowStatsService) GetExecutionEfficiency(
	ctx context.Context,
	teamID *uuid.UUID,
	days int,
) (map[string]interface{}, error) {
	stats, err := s.statsStore.GetDailyStats(ctx, teamID, days)
	if err != nil {
		return nil, err
	}

	// è®¡ç®—å¹³å‡å€¼
	totalExecutes := 0
	totalDuration := 0.0
	totalSuccess := 0
	totalSuccessDuration := 0.0

	for _, stat := range stats {
		totalExecutes += stat.TotalExecutes
		totalDuration += stat.AvgDuration * float64(stat.TotalExecutes)
		totalSuccess += stat.SuccessExecutes
		// å‡è®¾æˆåŠŸä»»åŠ¡çš„å¹³å‡æ—¶é•¿ä¸æ€»å¹³å‡æ—¶é•¿ç›¸åŒï¼ˆç®€åŒ–è®¡ç®—ï¼‰
		totalSuccessDuration += stat.AvgDuration * float64(stat.SuccessExecutes)
	}

	avgDuration := 0.0
	avgSuccessDuration := 0.0
	if totalExecutes > 0 {
		avgDuration = totalDuration / float64(totalExecutes)
	}
	if totalSuccess > 0 {
		avgSuccessDuration = totalSuccessDuration / float64(totalSuccess)
	}

	return map[string]interface{}{
		"average_duration":         fmt.Sprintf("%.2f", avgDuration),
		"average_success_duration": fmt.Sprintf("%.2f", avgSuccessDuration),
		"total_executed":           totalExecutes,
		"total_success":            totalSuccess,
	}, nil
}

// GetWorkflowRanking è·å–Workflowæ’è¡Œæ¦œ
func (s *WorkflowStatsService) GetWorkflowRanking(
	ctx context.Context,
	teamID *uuid.UUID,
	days int,
) (map[string]interface{}, error) {
	ranking, err := s.statsStore.GetWorkflowRanking(ctx, teamID, days, 10)
	if err != nil {
		return nil, err
	}

	return map[string]interface{}{
		"data":           ranking,
		"workflow_count": len(ranking),
	}, nil
}

// GetTimeDistribution è·å–æ—¶é—´åˆ†å¸ƒç»Ÿè®¡
// è¿”å›æŒ‰æ˜ŸæœŸå‡ çš„æ‰§è¡Œåˆ†å¸ƒ
func (s *WorkflowStatsService) GetTimeDistribution(
	ctx context.Context,
	teamID *uuid.UUID,
	days int,
) (map[string]interface{}, error) {
	stats, err := s.statsStore.GetDailyStats(ctx, teamID, days)
	if err != nil {
		return nil, err
	}

	// æŒ‰æ˜ŸæœŸå‡ ç»Ÿè®¡
	weekdayMap := make(map[string]*DailyAgg)
	weekdayNames := []string{"å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"}

	for _, stat := range stats {
		weekday := int(stat.StatDate.Weekday())
		if weekday == 0 {
			weekday = 7 // å‘¨æ—¥
		}
		weekdayName := weekdayNames[weekday-1]

		if agg, exists := weekdayMap[weekdayName]; exists {
			agg.Total += stat.TotalExecutes
			agg.Success += stat.SuccessExecutes
			agg.Failed += stat.FailedExecutes
		} else {
			weekdayMap[weekdayName] = &DailyAgg{
				Date:    weekdayName,
				Total:   stat.TotalExecutes,
				Success: stat.SuccessExecutes,
				Failed:  stat.FailedExecutes,
			}
		}
	}

	// è½¬æ¢ä¸ºåˆ—è¡¨ï¼ˆä¿æŒé¡ºåºï¼‰
	var weekdayData []map[string]interface{}
	for _, name := range weekdayNames {
		if agg, exists := weekdayMap[name]; exists {
			weekdayData = append(weekdayData, map[string]interface{}{
				"weekday":  name,
				"executed": agg.Total,
				"success":  agg.Success,
				"failed":   agg.Failed,
			})
		}
	}

	return map[string]interface{}{
		"weekday": weekdayData,
	}, nil
}

// GetPeriodComparison è·å–æ—¶é—´æ®µå¯¹æ¯”
// è¿”å›æœ¬å‘¨vsä¸Šå‘¨ã€æœ¬æœˆvsä¸Šæœˆçš„å¯¹æ¯”æ•°æ®
func (s *WorkflowStatsService) GetPeriodComparison(
	ctx context.Context,
	teamID *uuid.UUID,
) (map[string]interface{}, error) {
	result := make(map[string]interface{})
	now := time.Now()

	// ========== æœ¬å‘¨ vs ä¸Šå‘¨ ==========
	weekday := int(now.Weekday())
	if weekday == 0 {
		weekday = 7
	}
	thisWeekStart := now.AddDate(0, 0, -(weekday - 1)).Truncate(24 * time.Hour)
	lastWeekStart := thisWeekStart.AddDate(0, 0, -7)
	lastWeekEnd := thisWeekStart

	// ç»Ÿè®¡æœ¬å‘¨
	thisWeekStats, _ := s.statsStore.GetAggregateStats(ctx, teamID, thisWeekStart, now)
	// ç»Ÿè®¡ä¸Šå‘¨
	lastWeekStats, _ := s.statsStore.GetAggregateStats(ctx, teamID, lastWeekStart, lastWeekEnd)

	result["weekly"] = map[string]interface{}{
		"this_week": thisWeekStats,
		"last_week": lastWeekStats,
	}

	// ========== æœ¬æœˆ vs ä¸Šæœˆ ==========
	thisMonthStart := time.Date(now.Year(), now.Month(), 1, 0, 0, 0, 0, time.Local)
	lastMonthStart := thisMonthStart.AddDate(0, -1, 0)
	lastMonthEnd := thisMonthStart

	// ç»Ÿè®¡æœ¬æœˆ
	thisMonthStats, _ := s.statsStore.GetAggregateStats(ctx, teamID, thisMonthStart, now)
	// ç»Ÿè®¡ä¸Šæœˆ
	lastMonthStats, _ := s.statsStore.GetAggregateStats(ctx, teamID, lastMonthStart, lastMonthEnd)

	result["monthly"] = map[string]interface{}{
		"this_month": thisMonthStats,
		"last_month": lastMonthStats,
	}

	return result, nil
}
